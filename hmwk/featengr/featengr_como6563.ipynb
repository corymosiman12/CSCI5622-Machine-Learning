{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Homework \n",
    "***\n",
    "**Name**: $<$Cory Mosiman$>$ \n",
    "\n",
    "**Kaggle Username**: $<$CoryMosiman$>$\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 23rd**. Additionally, you must make at least one submission to the **Kaggle** competition before it closes at **4:59pm on Friday February 23rd**. Submit only this Jupyter notebook to Moodle. Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "When people are discussing popular media, there’s a concept of spoilers. That is, critical information about the plot of a TV show, book, or movie that “ruins” the experience for people who haven’t read / seen it yet.\n",
    "\n",
    "The goal of this assignment is to do text classification on forum posts from the website [tvtropes.org](http://tvtropes.org/), to predict whether a post is a spoiler or not. We'll be using the logistic regression classifier provided by sklearn.\n",
    "\n",
    "Unlike previous assignments, the code provided with this assignment has all of the functionality required. Your job is to make the functionality better by improving the features the code uses for text classification.\n",
    "\n",
    "**NOTE**: Because the goal of this assignment is feature engineering, not classification algorithms, you may not change the underlying algorithm or it's parameters\n",
    "\n",
    "This assignment is structured in a way that approximates how classification works in the real world: Features are typically underspecified (or not specified at all). You, the data digger, have to articulate the features you need. You then compete against others to provide useful predictions.\n",
    "\n",
    "It may seem straightforward, but do not start this at the last minute. There are often many things that go wrong in testing out features, and you'll want to make sure your features work well once you've found them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle In-Class Competition \n",
    "***\n",
    "\n",
    "In addition to turning in this notebook on Moodle, you'll also need to submit your predictions on Kaggle, an online tournament site for machine learning competitions. The competition page can be found here:  \n",
    "\n",
    "[https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018](https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018)\n",
    "\n",
    "Additionally, a private invite link for the competition has been posted to Piazza. \n",
    "\n",
    "The starter code below has a `model_predict` method which produces a two column CSV file that is correctly formatted for Kaggle (predictions.csv). It should have the example Id as the first column and the prediction (`True` or `False`) as the second column. If you change this format your submissions will be scored as zero accuracy on Kaggle. \n",
    "\n",
    "**Note**: You may only submit **THREE** predictions to Kaggle per day.  Instead of using the public leaderboard as your sole evaluation processes, it is highly recommended that you perform local evaluation using a validation set or cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 1: Feature Engineering \n",
    "***\n",
    "\n",
    "The `FeatEngr` class is where the magic happens.  In it's current form it will read in the training data and vectorize it using simple Bag-of-Words.  It then trains a model and makes predictions.  \n",
    "\n",
    "25 points of your grade will be generated from your performance on the the classification competition on Kaggle. The performance will be evaluated on accuracy on the held-out test set. Half of the test set is used to evaluate accuracy on the public leaderboard.  The other half of the test set is used to evaluate accuracy on the private leaderboard (which you will not be able to see until the close of the competition). \n",
    "\n",
    "You should be able to significantly improve on the baseline system (i.e. the predictions made by the starter code we've provided) as reported by the Kaggle system.  Additionally, the top **THREE** students from the **PRIVATE** leaderboard at the end of the contest will receive 5 extra credit points towards their Problem 1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatEngr:\n",
    "    def __init__(self):\n",
    "        \n",
    "        from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "        \n",
    "#         TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "        self.vectorizer = CountVectorizer(stop_words = 'english', \n",
    "#                                           token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                          ngram_range = (1,4))\n",
    "        self.tfidf = TfidfTransformer()\n",
    "        self.tfidf2 = TfidfTransformer()\n",
    "        self.alpha_num = AlphaNumTransformer()\n",
    "\n",
    "    def build_train_features(self, examples, subset = False):\n",
    "        \"\"\"\n",
    "        Method to take in training text features and do further feature engineering \n",
    "        Most of the work in this homework will go here, or in similar functions  \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "\n",
    "#         subset = 20\n",
    "        \n",
    "        omdb_df = self.omdb_features(examples, subset = subset)\n",
    "#         print('Examples shape before: {} omdb_df shape before: {}'.format(examples.shape, omdb_df.shape))\n",
    "        omdb_df.to_csv('omdb_df.csv')\n",
    "#         print('omdb_df index: {}, columns {}'.format(omdb_df.index, omdb_df.columns))\n",
    "#         print('examples index: {}, columns {}'.format(examples.index, examples.columns))\n",
    "        examples = examples.join(omdb_df, on = 'page')\n",
    "        examples.to_csv('line_49_examples.csv')\n",
    "#         print('Examples shape after: {} '.format(examples.shape))\n",
    "        examples = examples.drop(['unique_sep'], axis = 1)\n",
    "#         print(examples.info())\n",
    "        return examples\n",
    "\n",
    "    def month_to_month(self, a):\n",
    "#         print('month_to_mohth')\n",
    "        months_map = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr':4, 'May': 5, 'June': 6, 'July': 7,\n",
    "                 'Aug':8, 'Sep': 9, 'Oct':10, 'Nov':11, 'Dec': 12, 'None': np.nan}\n",
    "        try:\n",
    "            return months_map[a]\n",
    "        except:\n",
    "            return a\n",
    "\n",
    "    def separate_titles(self, column):\n",
    "        import re\n",
    "        '''\n",
    "        Used to separate the title at capital letters for easier passing into get request.\n",
    "        '''\n",
    "        sep_column = re.findall('[A-Z]{1}[a-z]{1,}[^A-Z]*', column)\n",
    "        if len(sep_column) > 1:\n",
    "            return(' '.join(sep_column))\n",
    "        else:\n",
    "            return(column)\n",
    "\n",
    "    def omdb_features(self, examples, subset):\n",
    "        '''\n",
    "        This method is meant to find additional features from the Open Movie Database (OMDB).\n",
    "        An API key was obtained and used to get potentially insightful additional features by using\n",
    "        the movie title.\n",
    "        '''\n",
    "\n",
    "        unique_titles = pd.DataFrame({'unique_unsep':pd.unique(examples['page'])})\n",
    "        unique_titles['unique_sep'] = unique_titles['unique_unsep'].apply(self.separate_titles)\n",
    "        unique_titles.set_index('unique_unsep', inplace = True)\n",
    "\n",
    "        print('There are {} unique titles'.format(unique_titles.shape[0]))\n",
    "\n",
    "        # Define what we would like to get back from OMDB\n",
    "        self.new_feat_keys = ['Runtime', 'Released','imdbVotes', 'imdbRating',\n",
    "                         'Genre', 'Rated', 'Type']\n",
    "#         self.new_feat_keys = ['Runtime', 'imdbVotes','imdbRating','Genre', 'Type']\n",
    "\n",
    "        # use subset if testing API\n",
    "        if subset:\n",
    "            print('Taking a subset of {} from the total number of unique_titles'.format(subset))\n",
    "            unique_titles = unique_titles.sample(subset)\n",
    "\n",
    "        # instantiate new dataframe\n",
    "        new_feat_df = pd.DataFrame(columns = self.new_feat_keys)\n",
    "\n",
    "        # make API call, \n",
    "        sample2 = unique_titles['unique_sep'].apply(self.call_omdb)\n",
    "#         print(type(sample2), sample2.head())\n",
    "\n",
    "        ###\n",
    "        # call returns \n",
    "        new_feat_df = pd.concat(sample2.tolist(), ignore_index=True)\n",
    "\n",
    "        # create 3 new columns from released date for Day, Month, Year\n",
    "        if 'Released' in self.new_feat_keys:\n",
    "            new = pd.DataFrame(new_feat_df['Released'].str.split(' ').values.tolist()).astype('object')\n",
    "            new.columns = ['num_day','num_month','num_year']\n",
    "            new['num_month'] = new['num_month'].apply(self.month_to_month)\n",
    "\n",
    "            # replace 'None' with NaN\n",
    "            new.fillna(value=np.nan, inplace = True)\n",
    "            new_feat_df = pd.concat([new_feat_df, new], axis = 1)\n",
    "            \n",
    "        new_feat_df.index = unique_titles.index\n",
    "        new_feat_df.to_csv('line_119_new_feat_df.csv')\n",
    "        \n",
    "        print('shape of sample: {}\\shape of new_feat_df: {}'.format(unique_titles.shape, new_feat_df.shape))\n",
    "        final = pd.concat([unique_titles, new_feat_df], axis = 1)\n",
    "\n",
    "        # convert desired columns to numeric data type\n",
    "        numeric_cols = ['imdbVotes', 'Runtime','imdbRating','num_day','num_month','num_year']\n",
    "        self.numeric_cols_current = list(set(self.new_feat_keys).intersection(numeric_cols))\n",
    "        final[self.numeric_cols_current] = final[self.numeric_cols_current].apply(lambda x: pd.to_numeric(x.astype(str).str.replace(',',''), \n",
    "                                                                                errors='coerce'))\n",
    "        \n",
    "        final.to_csv('line_130_final_df.csv')\n",
    "        # combine all text data into one 'text' column, dropping others\n",
    "        text_cols = ['Genre', 'Rated','Type', 'Released']\n",
    "        self.text_cols_current = list(set(self.new_feat_keys).intersection(text_cols))\n",
    "        print('self.text_cols_current type: {}'.format(type(self.text_cols_current)))\n",
    "        if 'Genre' in self.text_cols_current:\n",
    "            print('Genre in self.text_cols_current')\n",
    "            final['Genre'] = final['Genre'].str.replace(', ', ' ')\n",
    "            final.to_csv('line_137_final.csv')\n",
    "       \n",
    "        final['omdb_features_text'] = final[self.text_cols_current].apply(lambda x: ' '.join(x), axis = 1)\n",
    "        final.to_csv('line_140_final.csv')\n",
    "        print('final type: {}'.format(type(final)))\n",
    "#         final = final.drop(self.text_cols_current, axis = 1)\n",
    "\n",
    "#         from datetime import datetime\n",
    "        \n",
    "#         save_file = datetime.now().strftime('%Y-%m-%d %H_%M') + 'api_response.csv'\n",
    "#         final.to_csv(save_file)\n",
    "        return final\n",
    "\n",
    "\n",
    "    def call_omdb(self, title):\n",
    "        '''\n",
    "        This method actually performs the call to the OMDB API, checks if the response is valid \n",
    "        and has content, and returns the potentially useful features defined by ```find_these```.\n",
    "        '''\n",
    "        import requests\n",
    "        import os\n",
    "        import json\n",
    "\n",
    "        omdb_api_key = os.environ['OMDB_API_KEY']\n",
    "        omdb_base_url = 'http://www.omdbapi.com/'\n",
    "        parameters = {'apikey': omdb_api_key,\n",
    "                     't': title,\n",
    "                     'r': 'json'}\n",
    "        response = requests.get(omdb_base_url, params = parameters)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            return_this = ['NaN']*len(self.new_feat_keys)\n",
    "            return pd.DataFrame([return_this], columns=self.new_feat_keys)\n",
    "\n",
    "        elif response.status_code == 200:\n",
    "            resp_dict = json.loads(response.text)\n",
    "            if resp_dict['Response'] == 'True':\n",
    "                try:\n",
    "                    resp_dict['Runtime'] = resp_dict['Runtime'].split(' ')[0]\n",
    "                except:\n",
    "                    pass\n",
    "                new_feat_values = [resp_dict[feature] for feature in self.new_feat_keys]\n",
    "                return pd.DataFrame([new_feat_values], columns=self.new_feat_keys)\n",
    "\n",
    "            else:\n",
    "                return_this = ['NaN']*len(self.new_feat_keys)\n",
    "                return pd.DataFrame([return_this], columns=self.new_feat_keys)\n",
    "    \n",
    "    def get_test_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in test text features and transform the same way as train features \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        omdb_df = self.omdb_features(examples, subset = subset)\n",
    "        examples = examples.join(omdb_df, on = 'page')\n",
    "\n",
    "        return self.vectorizer.transform(examples)\n",
    "\n",
    "    def show_top10(self):\n",
    "        \"\"\"\n",
    "        prints the top 10 features for the positive class and the \n",
    "        top 10 features for the negative class. \n",
    "        \"\"\"\n",
    "        feature_names = np.asarray(self.vectorizer.get_feature_names())\n",
    "        top10 = np.argsort(self.logreg.coef_[0])[-10:]\n",
    "        bottom10 = np.argsort(self.logreg.coef_[0])[:10]\n",
    "        print(\"Pos: %s\" % \" \".join(feature_names[top10]))\n",
    "        print(\"Neg: %s\" % \" \".join(feature_names[bottom10]))\n",
    "    \n",
    "    \n",
    "    def train_model(self, random_state=1234):\n",
    "        \"\"\"\n",
    "        Method to read in training data from file, and \n",
    "        train Logistic Regression classifier. \n",
    "        \n",
    "        :param random_state: seed for random number generator \n",
    "        \"\"\"\n",
    "        \n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        from sklearn.feature_selection import SelectKBest\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.pipeline import FeatureUnion\n",
    "        from sklearn.preprocessing import FunctionTransformer\n",
    "        from sklearn.preprocessing import MaxAbsScaler\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.preprocessing import Normalizer \n",
    "        from sklearn.preprocessing import QuantileTransformer\n",
    "        from sklearn.preprocessing import Imputer\n",
    "        \n",
    "        # load data \n",
    "        dfTrain = pd.read_csv(\"../data/spoilers/train.csv\")\n",
    "        self.X_train = self.build_train_features(dfTrain.loc[:,['sentence', 'trope', 'page']])\n",
    "        \n",
    "        ####################################################################\n",
    "        # train logistic regression model.  !!You MAY NOT CHANGE THIS!! \n",
    "#         self.logreg = LogisticRegression(random_state=random_state)\n",
    "#         self.logreg.fit(self.X_train, self.y_train)\n",
    "        ####################################################################\n",
    "        \n",
    "        self.y_train = np.array(dfTrain[\"spoiler\"], dtype=int)\n",
    "        \n",
    "        # define simple lambda functions to extract str and float objects separately\n",
    "        get_text = FunctionTransformer(lambda x: self.make_one_text(x[['sentence','omdb_features_text']]), \n",
    "                                       validate = False)\n",
    "        get_nums = FunctionTransformer(lambda x: x[self.numeric_cols_current], validate = False)\n",
    "        get_counts = FunctionTransformer(lambda x: self.get_counts(x), validate = False)\n",
    "        get_text2 = FunctionTransformer(lambda x: self.make_one_text(x['sentence']), \n",
    "                                       validate = False)\n",
    "        \n",
    "        # Extract only text values, perform vectorizer and tfidf\n",
    "        text_pipe1 = Pipeline([\n",
    "            ('get_text', get_text),\n",
    "            ('vectorizer', self.vectorizer),\n",
    "            ('tfidf', self.tfidf)\n",
    "        ])\n",
    "        \n",
    "        text_pipe2 = Pipeline([\n",
    "            ('get_text',get_text2),\n",
    "            ('alpha_nums', self.alpha_num),\n",
    "#             ('tfidf2', self.tfidf2)\n",
    "        ])\n",
    "        \n",
    "        # \n",
    "        count_pipe = Pipeline([\n",
    "            ('count_pipe', get_counts),\n",
    "            ('impute', Imputer()),\n",
    "            ('scale', StandardScaler)\n",
    "        ])\n",
    "        # Extract only numerical values, perform pipelined operations\n",
    "#         num_pipe = Pipeline([\n",
    "#             ('get_nums', get_nums),\n",
    "#             ('impute', Imputer()),\n",
    "#             ('scale', MaxAbsScaler)\n",
    "#         ])\n",
    "        \n",
    "        # create pipeline for model, perform union on text and numerical pipelines returns,\n",
    "        # and pass to log reg\n",
    "        self.logreg = Pipeline([\n",
    "            ('union', FeatureUnion(\n",
    "                transformer_list = [\n",
    "                    ('count', count_pipe),\n",
    "                    ('text1', text_pipe1),\n",
    "#                     ('text2', text_pipe2)\n",
    "#                     ('nums', num_pipe)\n",
    "                ]\n",
    "            )),\n",
    "            ('scale', Normalizer()),\n",
    "            ('log_reg', LogisticRegression(random_state=random_state))\n",
    "        ])\n",
    "        \n",
    "        print('Right before self.logreg.fit: x_train_shape {}\\\n",
    "              y_train_shape {}'.format(self.X_train.shape, self.y_train.shape))\n",
    "              \n",
    "        self.X_train.to_csv('line_271_self_x_train.csv')\n",
    "        self.logreg.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        print('Train set accuracy: {}'.format(self.logreg.score(self.X_train, self.y_train)))\n",
    "        scores = cross_val_score(self.logreg, self.X_train, self.y_train, cv = 5)\n",
    "        print(scores)\n",
    "        print(\"Mean Accuracy in Cross-Validation = {:.3f}\".format(scores.mean()))\n",
    "    \n",
    "    def make_one_text(self, df):\n",
    "        print('make_one_text_function')\n",
    "        df.to_csv('line_279_make_one_text.csv')\n",
    "        df['make_one_text'] = df.apply(lambda x: ' '.join(x), axis = 1)\n",
    "        df = pd.DataFrame(df['make_one_text'], columns = ['make_one_text'])\n",
    "        df.to_csv('line_283_make_one_text.csv')\n",
    "        return list(df['make_one_text'])\n",
    "        \n",
    "    def get_counts(self, df):\n",
    "\n",
    "        import re\n",
    "        from scipy.sparse import csr_matrix\n",
    "        \n",
    "        # instantiate empty dataframe\n",
    "        cols = ['double_spaces','non_alpha','total_length']\n",
    "        counts_df = pd.DataFrame(columns = cols)\n",
    "        \n",
    "        # Count the number of double spaces\n",
    "        counts_df['double_spaces'] = df['sentence'].apply(lambda x: len(re.findall(r'  ', x)))\n",
    "        \n",
    "        # count non alpha numeric characters\n",
    "        counts_df['non_alpha'] = \\\n",
    "            df['sentence'].apply(lambda x: len([found for found in \\\n",
    "                                                re.findall(r'[^a-zA-Z0-9]', x) if not found == ' ']))\n",
    "            \n",
    "        # string length\n",
    "        counts_df['total_length'] = df['sentence'].str.len()\n",
    "        \n",
    "        # number of genres present from APIcall\n",
    "        counts_df['num_genres'] = \\\n",
    "            df['Genre'].apply(lambda x: len([found for found in \\\n",
    "                                             re.findall(r' ', x)]) + 1)\n",
    "            \n",
    "        counts_df['Runtime'] = df['Runtime']\n",
    "#         counts_df['imdbVotes'] = df['imdbVotes']\n",
    "#         counts_df['imdbRating'] = df['imdbRating']\n",
    "#         counts_df.to_csv('line_298_get_counts.csv')\n",
    "        print('counts_df.shape: {} counts_df.values.shape: {}'.format(counts_df.shape, \n",
    "                                                                      counts_df.values.shape))\n",
    "#         print(list(counts_df.values))\n",
    "        return counts_df\n",
    "#         return list(counts_df.values)\n",
    "#         return counts_df['Runtime']  NOPE\n",
    "#         return counts_df.values.reshape(-1,1)  NOPE\n",
    "#         return counts_df.reshape(-1,1)  NOPE\n",
    "        \n",
    "    def model_predict(self):\n",
    "        \"\"\"\n",
    "        Method to read in test data from file, make predictions\n",
    "        using trained model, and dump results to file \n",
    "        \"\"\"\n",
    "        \n",
    "        # read in test data \n",
    "        dfTest  = pd.read_csv(\"../data/spoilers/test.csv\")\n",
    "        \n",
    "        # featurize test data \n",
    "        self.X_test = self.build_train_features(dfTest[['sentence', 'trope', 'page']])\n",
    "        \n",
    "        # make predictions on test data \n",
    "        pred = self.logreg.predict(self.X_test)\n",
    "        \n",
    "        # dump predictions to file for submission to Kaggle  \n",
    "        pd.DataFrame({\"spoiler\": np.array(pred, dtype=bool)}).to_csv(\"prediction.csv\", \n",
    "                                                                     index=True, index_label=\"Id\")\n",
    "        \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class AlphaNumTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        import numpy as np \n",
    "        from scipy.sparse import csr_matrix\n",
    "        \n",
    "        alpha_nums = ['.', '!', '@', '#', '$','%','^','&','*','-',' ', '  ', '?']\n",
    "         \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples), len(alpha_nums)))\n",
    "        \n",
    "        # Loop over examples and count letters \n",
    "        for ii, x in enumerate(examples):\n",
    "            X[ii,:] = np.array([x.count(alpha_nums) for alpha_num in alpha_nums])\n",
    "            \n",
    "        return csr_matrix(X) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 679 unique titles\n",
      "shape of sample: (679, 1)\\shape of new_feat_df: (679, 10)\n",
      "self.text_cols_current type: <class 'list'>\n",
      "Genre in self.text_cols_current\n",
      "final type: <class 'pandas.core.frame.DataFrame'>\n",
      "Right before self.logreg.fit: x_train_shape (11970, 14)              y_train_shape (11970,)\n",
      "counts_df.shape: (11970, 5) counts_df.values.shape: (11970, 5)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0f5b2579acbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train your Logistic Regression classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1230\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Shows the top 10 features for each class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-3e28c1cd35df>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, random_state)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'line_271_self_x_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train set accuracy: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m             delayed(_fit_transform_one)(trans, weight, X, y,\n\u001b[1;32m    738\u001b[0m                                         **fit_params)\n\u001b[0;32m--> 739\u001b[0;31m             for name, trans, weight in self._iter())\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlast_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "# Instantiate the FeatEngr class\n",
    "feat = FeatEngr()\n",
    "\n",
    "# Train your Logistic Regression classifier \n",
    "feat.train_model(random_state=1230)\n",
    "\n",
    "# Shows the top 10 features for each class \n",
    "# feat.show_top10()\n",
    "\n",
    "# Make prediction on test data and produce Kaggle submission file \n",
    "# feat.model_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 2: Motivation and Analysis \n",
    "***\n",
    "\n",
    "The job of the written portion of the homework is to convince the grader that:\n",
    "\n",
    "- Your new features work\n",
    "- You understand what the new features are doing\n",
    "- You had a clear methodology for incorporating the new features\n",
    "\n",
    "Make sure that you have examples and quantitative evidence that your features are working well. Be sure to explain how you used the data (e.g., did you have a validation set? did you do cross-validation?) and how you inspected the results. In addition, it is very important that you show some kind of an **error analysis** throughout your process.  That is, you should demonstrate that you've looked at misclassified examples and put thought into how you can craft new features to improve your model. \n",
    "\n",
    "A sure way of getting a low grade is simply listing what you tried and reporting the Kaggle score for each. You are expected to pay more attention to what is going on with the data and take a data-driven approach to feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### OMDB API\n",
    "\n",
    "After looking at the original dataset and understanding the problem at hand, I began researching spoilers.  I came across the paper _Spoiler Alert: Machine Learning Approaches to Detect Social Media Posts with Revelatory Information_, which discusses the role of additional metadata (Genre, Length, First Aired, Episodes, Country) for creating a more robust model.  They were able to boost their spoiler detection accuracy from 60%, using only unigrams and bigrams, to 67%, using the additional aforementioned features in addition to their original baseline model.  With that in mind, I began researching for access to movie metadata.  I found the [OMDB API](http://www.omdbapi.com/), which provides metadata information for movies, shows, etc.  A sample response from their API is below:  \n",
    "```json\n",
    "{\n",
    "    \"Title\": \"America's Funniest Home Videos\",\n",
    "    \"Year\": \"1989–\",\n",
    "    \"Rated\": \"TV-PG\",\n",
    "    \"Released\": \"26 Nov 1989\",\n",
    "    \"Runtime\": \"30 min\",\n",
    "    \"Genre\": \"Comedy, Family, Reality-TV\",\n",
    "    \"Director\": \"N/A\",\n",
    "    \"Writer\": \"N/A\",\n",
    "    \"Actors\": \"Jess Harnell, Tom Bergeron, Bob Saget, Ernie Anderson\",\n",
    "    \"Plot\": \"Viewers from around America send in home videos with comedic moments.\",\n",
    "    \"Language\": \"English\",\n",
    "    \"Country\": \"USA\",\n",
    "    \"Awards\": \"4 wins & 6 nominations.\",\n",
    "    \"Poster\": \"https://images-na.ssl-images-amazon.com/images/M/MV5BMTY3MDkzMDE4Nl5BMl5BanBnXkFtZTcwMjM3ODQzMQ@@._V1_SX300.jpg\",\n",
    "    \"Ratings\": [\n",
    "        {\n",
    "            \"Source\": \"Internet Movie Database\",\n",
    "            \"Value\": \"6.2/10\"\n",
    "        }\n",
    "    ],\n",
    "    \"Metascore\": \"N/A\",\n",
    "    \"imdbRating\": \"6.2\",\n",
    "    \"imdbVotes\": \"4,187\",\n",
    "    \"imdbID\": \"tt0098740\",\n",
    "    \"Type\": \"series\",\n",
    "    \"totalSeasons\": \"27\",\n",
    "    \"Response\": \"True\"\n",
    "}```\n",
    "\n",
    "Using the suggestions from the _Spoiler Alert_ paper, as well as what I thought might be useful, I decided to extract the following features:\n",
    "\n",
    "```python\n",
    "new_features = ['Runtime', 'Released','imdbVotes', 'imdbRating','Genre', 'Rated', 'Type']\n",
    "```\n",
    "\n",
    "> **Runtime** (numeric): Potentially shows (28-30 mins) are more/less susceptible to spoilers compared to films (70-120 mins).  \n",
    "> **Released** (numeric): The resesarchers found that newer movies were more likely to have spoilers than older movies.  \n",
    "> **imdbVotes** (numeric): Potentially movies with more/less votes are more spoiled.  \n",
    "> **imdbRating** (numeric): Same as above.  \n",
    "> **Genre** (text): Could the genre of the movie effect the spoiler rate?  \n",
    "> **Rated** (text): Potentially R-rated movies are spoiled more often than G-rated movies.  \n",
    "> **Type** (text): Maybe movies are more often spoiled than shows.\n",
    "\n",
    "I wrote a few functions to ingest, sort, and process this data from the API.  One of the shortcomings of the method is that, depending on the syntax, some movies/shows did not have data available.  In those cases, all of the fields were labeled with `NaN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "***\n",
    "**`CountVectorizer()`**  \n",
    "\n",
    "Using the concepts learned from the _Lecture 3: Logistic Regression and Text Models_ notebook, I decided to use a bag-of-words approach with the ```CountVectorizer()``` class, which creates a sparse representation of a term-frequency matrix.  The point of a term-frequency matrix is to scan a corpus and come up with a matrix representation of the associated words in the corpus.  It is simply the application of a one-hot-encoding scheme to strings, texts, etc., with a higher level API for implementing n-grams, stop-words, and other features.  \n",
    "\n",
    "***\n",
    "\n",
    "**`TfidfTransformer()`**\n",
    "\n",
    "The ```TfidfTransformer()``` class stands for term-frequency, inverse document frequency.  It is similar to ```CountVectorizer()``` in that it creates a bag-of-words (the term-frequency part) model for each document in a corpus. However, instead of representing only the count of each word in the corpus, it adjusts this with the following:  \n",
    "\n",
    "$$\n",
    "\\texttt{idf(t)} = \\ln ~ \\frac{\\textrm{total # documents}}{\\textrm{1 + # documents with term }t}\n",
    " = \\ln ~ \\frac{\\left|~D~\\right|}{1 + \\left|\\{~d: ~ t \\in d\\}~\\right|}\n",
    "$$\n",
    "\n",
    "Basically, it is calculating the importance of each individual word based on its frequency in the corpus.\n",
    "\n",
    "***\n",
    "\n",
    "**`AlphaNumTransformer()`**\n",
    "\n",
    "I created the ```AlphaNumTransformer()``` to basically create a one-hot-encoding scheme for all non-alphanumeric transformers, which I thought might provide additional information to the model.  This class was instantiated based on the ```XYZTransformer()``` class from the _Lecture 7: Feature Engineering_ notebook.  For instance, perhaps spoilers are more likely to use exclamation points in their comments.  The investigated alpha numeric characters are below.\n",
    "\n",
    "```python\n",
    "alpha_nums = ['.', '!', '@', '#', '$','%','^','&','*','-',' ', '  ', '?']```\n",
    "\n",
    "***\n",
    "\n",
    "**`get_counts()`**\n",
    "\n",
    "The original intention of the get_counts method was to extract the numerical data from the OMDB API response. The intention was for it to apply `lambda` functions to different elements of the dataframe to extract the following numerical/count features:  \n",
    "\n",
    "* Runtime (**unsuccessful**)\n",
    "* imdbVotes (**unsuccessful**)\n",
    "* imdbRating (**unsuccessful**)\n",
    "* comment length (successful)\n",
    "* alphanumeric characters (successful)\n",
    "* number of genres represented (successful)\n",
    "* number of double spaces (successful)\n",
    "    \n",
    "These were the original, and it was my plan to add more, however, I spent too much time trying to trouble shoot why I was unable to get all the above to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Code and Different Objects\n",
    "\n",
    "The above approach was implemented using both the ```Pipeline()``` and ```FeatureUnion()``` classes from the ```sklearn.pipeline``` library. A discussion of the different scikit learn objects is provided below.\n",
    "\n",
    "> [Estimators](http://scikit-learn.org/stable/tutorial/statistical_inference/settings.html#estimators-objects) are the main API implemented by scikit-learn for any object that learns from data.  An estimator will always have a `.fit()` and `.transform()` method, which takes a dataset (typically a 2-d array).  An estimator object can be any of the following: classification, regression, clustering, OR a transformer that extracts or filters useful features from the raw data.\n",
    "\n",
    "> [BaseEstimator](http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html) is the base class required for all estimators.  It consists of `get_params` and `set_params`.\n",
    "\n",
    "> [Transformers](http://scikit-learn.org/stable/data_transforms.html) are a type of estimator that have both `.fit()` and `.transform()` methods.  They are used for the following:\n",
    "\n",
    "> * Preprocess (i.e. normalization, standardization, imputation, etc.)\n",
    "> * Reduce (i.e. PCA)\n",
    "> * Expand (i.e. kernel approximation)\n",
    "> * Generate feature representations (i.e. feature extraction).\n",
    "\n",
    "> [TransformerMixin](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html) basically just adds the `.fit_transform()` method to any transformer.\n",
    "\n",
    "> [Pipeline](http://scikit-learn.org/stable/modules/pipeline.html#pipeline-chaining-estimators)( ) provides a way to chain multiple estimators (which include transformers) together.  Basically exposes the `.fit()` and `.transform()` methods of each of the underlying estimators into a single interface.\n",
    "\n",
    "> [FeatureUnion](http://scikit-learn.org/stable/modules/pipeline.html#featureunion-composite-feature-spaces)( ) provides a very simple method for joining features going through separate transformation steps, such as would happen for text data compared to numerical data.  For instance, text data would likely go through a `CountVectorizer()` to generate a feature representation of the different words, and then maybe a `TfidfTransformer()`.  However, numerical data would likely go through a a preprocessing step for standardization or normalization of the features.\n",
    "\n",
    "### Issues\n",
    "\n",
    "I encountered some issues that I was unsuccessful in resolving throughout the course of the assignment.  In particular, and what was most disappointing, I was unable to use any of the numerical data gathered from the OMDB API (as can be observed from all of the commented sections in the ```get_counts()``` method).  The main reason for writing the above section was to try to better understand why I was getting the following errors:\n",
    "\n",
    "`numpy ndarray object has no attribute fit`   \n",
    "`fit not found`  \n",
    "`ValueError: Expected 2D array, got 1D array instead...Reshape your data using array.reshape(-1,1)...`\n",
    "\n",
    "I still don't fully understand why my get_counts method works for some features but not others.  Although I now realize that an object in a pipeline should have both a `.fit()` and `.transform()` method in order to run successfully.\n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "Unfortunately, I spent too much time trying to get the numeric features to work that I didn't implement any solid error analysis into my approach.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints \n",
    "***\n",
    "\n",
    "- Don't use all the data until you're ready. \n",
    "\n",
    "- Examine the features that are being used.\n",
    "\n",
    "- Do error analyses.\n",
    "\n",
    "- If you have questions that aren’t answered in this list, feel free to ask them on Piazza.\n",
    "\n",
    "### FAQs \n",
    "***\n",
    "\n",
    "> Can I heavily modify the FeatEngr class? \n",
    "\n",
    "Totally.  This was just a starting point.  The only thing you cannot modify is the LogisticRegression classifier.  \n",
    "\n",
    "> Can I look at TV Tropes?\n",
    "\n",
    "In order to gain insight about the data yes, however, your feature extraction cannot use any additional data (beyond what I've given you) from the TV Tropes webpage.\n",
    "\n",
    "> Can I use IMDB, Wikipedia, or a dictionary?\n",
    "\n",
    "Yes, but you are not required to. So long as your features are fully automated, they can use any dataset other than TV Tropes. Be careful, however, that your dataset does not somehow include TV Tropes (e.g. using all webpages indexed by Google will likely include TV Tropes).\n",
    "\n",
    "> Can I combine features?\n",
    "\n",
    "Yes, and you probably should. This will likely be quite effective.\n",
    "\n",
    "> Can I use Mechanical Turk?\n",
    "\n",
    "That is not fully automatic, so no. You should be able to run your feature extraction without any human intervention. If you want to collect data from Mechanical Turk to train a classifier that you can then use to generate your features, that is fine. (But that’s way too much work for this assignment.)\n",
    "\n",
    "> Can I use a Neural Network to automatically generate derived features? \n",
    "\n",
    "No. This assignment is about your ability to extract meaningful features from the data using your own experimentation and experience.\n",
    "\n",
    "> What sort of improvement is “good” or “enough”?\n",
    "\n",
    "If you have 10-15% improvement over the baseline (on the Public Leaderboard) with your features, that’s more than sufficient. If you fail to get that improvement but have tried reasonable features, that satisfies the requirements of assignment. However, the extra credit for “winning” the class competition depends on the performance of other students.\n",
    "\n",
    "> Where do I start?  \n",
    "\n",
    "It might be a good idea to look at the in-class notebook associated with the Feature Engineering lecture where we did similar experiments. \n",
    "\n",
    "\n",
    "> Can I use late days on this assignment? \n",
    "\n",
    "You can use late days for the write-up submission, but the Kaggle competition closes at **4:59pm on Friday February 23rd**\n",
    "\n",
    "> Why does it say that the competition ends at 11:59pm when the assignment says 4:59pm? \n",
    "\n",
    "The end time/date are in UTC.  11:59pm UTC is equivalent to 4:59pm MST.  Kaggle In-Class does not allow us to change this. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
