{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Stochastic Gradient Descent\n",
    "***\n",
    "\n",
    "<img src=\"figs/mountains.jpg\" width=1100 height=50>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Problem 1: Unregularized Stochastic Gradient Ascent on Text Data \n",
    "***\n",
    "\n",
    "Suppose you have two messages, the first labeled positive (y = 1) and the second labeled negative (y = 0):\n",
    "\n",
    "| POS Message | NEG Message  | \n",
    "|:-----------:|:------------:|\n",
    "| AAAABBBC    |  BBCCCD      |\n",
    "\n",
    "**Q**: Your first task is to encode the messages into feature vectors using the Bag-of-Words approach.  You may assume that \"A\", \"B\", \"C\", and \"D\" are the  only symbols in the vocabulary.  Don't forget to add a feature corresponding to the **bias** term!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "x1 = np.array([1, 4, 3, 1, 0])\n",
    "y1 = 1\n",
    "x2 = np.array([1, 0, 2, 3, 1])\n",
    "y2 = 0\n",
    "w0 = np.zeros(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Your next task is to do one full pass through the data with **unregularized** SGA with the initial weight vector ${\\bf w}$ set to zero and the learning rate set to $\\eta = 1.0$.  You may assume that after the shuffle the order is POS then NEG.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933071490757153"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1: [ 0.5  2.   1.5  0.5  0. ]\n",
      "w2: [-0.49330715  2.         -0.4866143  -2.47992145 -0.99330715]\n"
     ]
    }
   ],
   "source": [
    "eta = 1\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "def gradient(w, x, y, eta):\n",
    "    a = sigmoid(np.dot(np.transpose(w),x))-y\n",
    "    w2 = w - eta*a*x\n",
    "    return w2\n",
    "\n",
    "w1 = gradient(w0, x1, y1, eta)\n",
    "w2 = gradient(w1, x2, y2, eta)\n",
    "print('w1: {}'.format(w1))\n",
    "print('w2: {}'.format(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: A More Efficient Regularization \n",
    "***\n",
    "\n",
    "Recall that we commonly include in the log-likelihood function a regularization term (on all weights except for the bias) of the form $\\lambda \\sum_{k=1}^D w_k^2$.   Incidentally, there are many forms of regularization.  This one is called L2-regularization because it can also be written as $\\lambda\\|{\\bf w}_{1:D}\\|_2^2$ where the norm here is the 2-norm or the *Euclidean* norm. For SGA this has the effect of including a term like $2\\lambda w_k$ in the $k^\\textrm{th}$ component of the gradient. \n",
    "\n",
    "**Q**: Why might this be terribly inefficient, especially in the context of text learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "The reason this could be very inefficient is because regularization occurs for every parameter, ${\\bf w}_k$, regardless of whether or not it is in the ${\\bf w}_k^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A workaround for this is to use a *slightly* different form of L2 regularization, which we now describe.  \n",
    "\n",
    "Consider the update step for the $k^\\textrm{th}$ component of the weight vector in SGA with regularization.  It looks like\n",
    "\n",
    "$$\n",
    "w_k \\leftarrow w_k - \\eta\\left([\\textrm{sigm}({\\bf w}^T{\\bf x}_i)-y_i]x_{ik} + 2\\lambda w_k \\right)\n",
    " = (w_k - \\eta[\\textrm{sigm}({\\bf w}^T{\\bf x}_i)-y_i]x_{ik}) - 2\\eta\\lambda w_k \n",
    "$$\n",
    "\n",
    "We're going to break the update up into two parts.  First we'll do the unregularized gradient update for $k$ corresponding only to **nonzero-features** in ${\\bf x}_i$: \n",
    "\n",
    "$$\n",
    "w_k \\leftarrow w_k - \\eta[\\textrm{sigm}({\\bf w}^T{\\bf x}_i)-y_i]x_{ik}\n",
    "$$\n",
    "\n",
    "And then (in theory) we'll do the regularization update as follows \n",
    "\n",
    "$$\n",
    "w_k \\leftarrow w_k - 2\\eta \\lambda w_k = w_k (1 - 2\\eta \\lambda)  \n",
    "$$\n",
    "\n",
    "**Q**: Look carefully at the two update steps.  Why is this not the same as standard L2-regularization? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "In the above equation, the $w_k$ used in the second update step is the $w_k$ from the first update step, when, in reality, it should be the original, un-updated version of $w_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so far this doesn't look very helpful.  It seems like we still need to do the second update step for nonzero-weights, even if the $k^\\textrm{th}$ feature is a zero-feature. It turns out that we can actually hold off on doing the regularization update for a weight until the time when we have to do a gradient update as well.  \n",
    "\n",
    "The property of the update that makes this possible is that the regularization update is **multiplicative**.  This means that doing **three** updates of the form \n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "~ w_k \\leftarrow w_k (1 - 2\\eta \\lambda) \\\\\n",
    "~ w_k \\leftarrow w_k (1 - 2\\eta \\lambda) \\\\\n",
    "~ w_k \\leftarrow w_k (1 - 2\\eta \\lambda) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "is equivalent to \n",
    "\n",
    "$$\n",
    "w_k \\leftarrow w_k (1 - 2\\eta \\lambda)^3\n",
    "$$\n",
    "\n",
    "This means that we can hold off on doing the regularization update for the $k^\\textrm{th}$ weight until we encounter a training example that has a $k^{\\textrm{th}}$ nonzero feature.  The regularization update is then performed where the **shrinkage factor** is raised to the power of the number of iterations since the feature was last updated. \n",
    "\n",
    "The only mechanism we need to do this is a data structure to keep track of the last iteration that a particular component of the weight vector was updated.  Typically this is done with a hash table.  In your next homework assignment you'll use a dictionary. \n",
    "\n",
    "This processes is called **Lazy Sparse Regularization** and you can read more about it in a slightly different context <a href=\"https://lingpipe.files.wordpress.com/2008/04/lazysgdregression.pdf\">here</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Redo Problem 1 with Lazy Sparse Regularization with $\\lambda = \\frac{1}{4}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "<br><br><br><br><br>\n",
    "<br><br><br><br><br>\n",
    "<br><br><br><br><br>\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
